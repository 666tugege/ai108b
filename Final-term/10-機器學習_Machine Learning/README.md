# 逻辑回归分类的优缺点及使用数据类型

决策树就是事先设定一棵树，树的所有非叶子节点都是由特征构成的，然后根据这个树自定而下问问题，直到把测试记录归类到叶子节点（最终的分类结果）。

# 范例：

根据动物颜色、长度等特征判断动物智商高低（分类结果）；

根据一系列特征判断给患者配什么眼镜等。

优点：

数据形式非常容易理解；可以处理不相关特征数据；计算复杂度不高；

缺点：

可能产生过渡匹配问题

适用数据类型：

数值型和标称型

介绍算法基本思想前，先聊一下信息增益（Information Gain），又叫熵（Entropy）。划分数据集前后信息发生的变化称之为信息增益。如果有多个特征可以用来划分数据集，我们只要计算出每个特征划分数据集后的信息增益，取增益最大的特征，就是最好的特征选择。

常见的决策树算法包括：Hunts以及基于Hunts的ID3、C4.5、CART等

# 算法基本思想

1）选取特征（特征即属性）

2）计算目标特征的信息熵Eg

3）分别计算每一个特征的信息熵Ei

4）Max(Eg-Ei)

判断出信息熵差值最大的特征i，把第i个特征作为最终的分类特征。

下面举例来说明这个公式：

假使说我们要研究狗的智商（目标属性），潜在的关联因素包括颜色和毛的长度。

颜色（color）	毛的长度（length）	智商（IQ）
black	长	高
white	长	高
white	短	高
white	短	低
3次出现“高”智商，1次出现“低智商”，所以目标属性IQ的信息熵：HIQ(D)=-(3/4)log2(3/4)-(1/4)log2(1/4)

color属性在取不同的值对应目标属性IQ的信息熵：

在color取值为black的时候，IQ只取“高”这个值：Hcolor(Dblack)=-(1/1)log2(1/1)
在color取值为white的时候，IQ取值有两个“高”，一个“低”：Hcolor(Dwhite)=-(1/3)log2(1/3)-(2/3)log2(2/3)
而color属性的整体信息熵是上述二者的加权平均值：Hcolor(D)=(1/4)Hcolor(Dblack)+(3/4)Hcolor(Dwhite)。同样可以求得Hlength(D)。

现在定义信息增益Gaincolor=HIQ(D)-Hcolor(D)，Gainlength=HIQ(D)-Hlength(D)，它是信息熵的有效减少量，值越高，说明目标属性IQ在参考属性处损失的信息熵越多，也就是失去的不确定性越多，那么在决策树进行分类的时候，这个参考属性应该越早作为决策的依据属性。

这个例子中如果Gainlength > Gaincolor，说明length比color要对IQ有更大的影响，所以决策树中，要先依据length来进行分类。

在实际应用中，往往引入一个“阈值”，当节点下的任意一分类所占百分比超过这个阈值时，就不再进行分类，以避免产生出过小的没有实际意义分类节点。

ID3算法也存在诸多不足，比如分类偏向于取值数量，只能处理离散数据等等问题。C4.5算法是对ID3的一个改进，但是总体来看，由于需要反复遍历数据，二者都是效率很低的算法。


# 参考资料
http://www.ryanzhang.info/archives/994
